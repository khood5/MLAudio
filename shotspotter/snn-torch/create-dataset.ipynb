{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893dd921-1942-4e44-83ae-1dab1fe06d15",
   "metadata": {},
   "source": [
    "# Setup\n",
    "This notebook is basically a mix of make_dataset.py from ../snn with encoding using delta modulation and adding waveform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdec943e-84a4-407f-a115-0f704cbb44b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/dev/neuromorphic/neurovenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import speech2spikes\n",
    "import torchaudio\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "import noisereduce\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import write_spikes_to_disk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b823c3e3-b02f-4383-b17b-32261bc229dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables for encoding\n",
    "MODE = 'dwt' # spec, dwt\n",
    "DATASET_CAP = 120\n",
    "FILEPATH = './data/null-dwt-wpndm.npz' # for saving\n",
    "DM_THRESHOLD = 0.05\n",
    "\n",
    "DWT_LEVELS = 7\n",
    "DWT_TIMESTEPS = 500\n",
    "\n",
    "SPEC_FREQ_BIN_COUNT = 25 # for spec mode\n",
    "\n",
    "PATH_GUNSHOT_SOUNDS = '/home/joao/dev/MLAudio/shotspotter/data/gunshots' \n",
    "PATH_GUNSHOT_INDEX = '/home/joao/dev/MLAudio/shotspotter/data/gunshotsNewIndex.csv'\n",
    "PATH_NOGUNSHOT_SOUNDS = '/home/joao/dev/MLAudio/shotspotter/data/genBackgrounds'\n",
    "\n",
    "### IMPORTANT: fix the part where we downsample the waveform, maybe use library to do it.\n",
    "### - only works now because coincidentally the bin size perfectly divides the sample count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c17a0ca-5d74-4c81-9f86-df2c8f65b4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 60 gunshot audio files\n",
      "We have 60 background only audio files\n"
     ]
    }
   ],
   "source": [
    "gunshot_file_paths = [PATH_GUNSHOT_SOUNDS+'/'+fn for fn in os.listdir(PATH_GUNSHOT_SOUNDS)][:DATASET_CAP//2]\n",
    "print(f'We have {len(gunshot_file_paths)} gunshot audio files')\n",
    "nogunshot_file_paths = [PATH_NOGUNSHOT_SOUNDS+'/'+fn for fn in os.listdir(PATH_NOGUNSHOT_SOUNDS)][:DATASET_CAP//2]\n",
    "print(f'We have {len(nogunshot_file_paths)} background only audio files')\n",
    "\n",
    "p1 = [(i, 1) for i in gunshot_file_paths]\n",
    "p2 = [(i, 0) for i in nogunshot_file_paths]\n",
    "\n",
    "pairs = p1+p2 # path to sound - label tuples\n",
    "random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3ca59-566a-410a-850f-22f2766141ea",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ced40e-12b6-40dc-ac71-470a8f596563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is shape time x batch x channels (from spectrogram or dwt spec)\n",
    "def posneg_delta(raw_spec_data):\n",
    "    delta = spikegen.delta(raw_spec_data, threshold=DM_THRESHOLD, off_spike=True)\n",
    "    \n",
    "    new_data = torch.zeros(delta.shape[0], delta.shape[1], delta.shape[2]*2, device=delta.device)\n",
    "\n",
    "    pos_mask = (delta == 1)\n",
    "    neg_mask = (delta == -1)\n",
    "\n",
    "    new_data[:, :, :delta.shape[2]] = pos_mask.to(torch.float32)\n",
    "    new_data[:, :, delta.shape[2]:] = neg_mask.to(torch.float32)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def to_spikes(paths_list, labels):\n",
    "    if MODE == 'spec':\n",
    "        all_spikes = []\n",
    "        all_waveforms = []\n",
    "        targets = np.array(labels)\n",
    "\n",
    "        p_count = 0\n",
    "        for p in paths_list:\n",
    "            # log\n",
    "            p_count += 1\n",
    "            if p_count % 100 == 0:\n",
    "                print(f'Done {p_count} samples.')\n",
    "            \n",
    "            samples, rate = torchaudio.load(p, normalize=True)\n",
    "            \n",
    "            if samples.shape[0] == 2: samples = samples[0, :]\n",
    "            else: samples = samples[0]\n",
    "            if(len(samples) < 24000):\n",
    "                samples = torch.cat((samples, torch.tensor([0])))\n",
    "\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "            \n",
    "            samples = torch.tensor(noisereduce.reduce_noise(y=samples, sr=rate)) # testing this because I had it on in the ResNet version dataset\n",
    "\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "\n",
    "            # freq bin count is nfft//2 + 1\n",
    "            spec_transform = torchaudio.transforms.Spectrogram(n_fft=(2*SPEC_FREQ_BIN_COUNT-2))\n",
    "\n",
    "            samples = samples.to(torch.float64)\n",
    "            spec = spec_transform(samples)\n",
    "\n",
    "            # convert waveform to spikes\n",
    "            timesteps = spec.shape[1]\n",
    "            waveform_timestep_len = (24000//timesteps) + 1\n",
    "\n",
    "            ts_waveform = [] # timesteps but compressed to time resolution of spectrogram output\n",
    "            current_t = waveform_timestep_len\n",
    "            while current_t <= 24000:\n",
    "                ts_waveform.append(samples[current_t-waveform_timestep_len: current_t].max())\n",
    "                current_t += waveform_timestep_len\n",
    "\n",
    "            if len(ts_waveform) < timesteps: # pad so dimensions match\n",
    "                ts_waveform.append(0) \n",
    "\n",
    "            # normalize because amplitude scales vary a lot (and we did this in resnet accidentally)\n",
    "            ts_waveform = torch.tensor(ts_waveform)\n",
    "            ts_waveform = (ts_waveform - ts_waveform.min()) / (ts_waveform.max() - ts_waveform.min())\n",
    "\n",
    "            # debug\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "            #plt.plot(np.linspace(0, len(ts_waveform), len(ts_waveform)), ts_waveform)\n",
    "\n",
    "            # convert compressed waveform into spikes using bins\n",
    "            waveform_spikes = []\n",
    "            num_bins = 20\n",
    "            bin_size = 1/num_bins\n",
    "            for w in ts_waveform:\n",
    "                waveform_spikes.append([0 for i in range(num_bins)])\n",
    "                waveform_spikes[-1][int(w//bin_size)] = 1\n",
    "\n",
    "            waveform_spikes = torch.tensor(waveform_spikes)\n",
    "\n",
    "            # debug\n",
    "            # fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "            # ax = fig.add_subplot(111)\n",
    "            # splt.raster(waveform_spikes, ax, s=5, c=\"black\")\n",
    "\n",
    "            all_spikes.append(spec)\n",
    "            all_waveforms.append(waveform_spikes)\n",
    "\n",
    "        # global max/min normalization\n",
    "        print('Looking for global max/min...')\n",
    "        global_min = 0\n",
    "        global_max = 0\n",
    "        for s in all_spikes:\n",
    "            if s.min() < global_min: global_min = s.min()\n",
    "            if s.max() > global_max: global_max = s.max()\n",
    "\n",
    "        # now normalize everything\n",
    "        print('Normalizing...')\n",
    "        for i in range(len(all_spikes)):\n",
    "            all_spikes[i] = (all_spikes[i]-global_min) / (global_max - global_min)\n",
    "\n",
    "        all_spikes = torch.stack(all_spikes)\n",
    "        all_waveforms = torch.stack(all_waveforms)\n",
    "\n",
    "        # delta modulate spectrogram\n",
    "        all_spikes = all_spikes.permute(2, 0, 1)\n",
    "        print('Running deltamod')\n",
    "        all_spikes_dm = posneg_delta(all_spikes)\n",
    "\n",
    "        # note: here, all_spikes_dm is (timestep, batch, neuron) and all_waveforms is (batch, timestep, neuron)\n",
    "        all_waveforms = all_waveforms.permute(1, 0, 2)\n",
    "\n",
    "        return torch.cat((all_spikes_dm, all_waveforms), dim=2), torch.tensor(labels)\n",
    "\n",
    "    # DWT MODE\n",
    "    elif MODE == 'dwt':\n",
    "        all_spikes = []\n",
    "        all_waveforms = []\n",
    "        targets = np.array(labels)\n",
    "\n",
    "        p_count = 0\n",
    "        for p in paths_list:\n",
    "            # log\n",
    "            p_count += 1\n",
    "            if p_count % 100 == 0:\n",
    "                print(f'Done {p_count} samples.')\n",
    "            \n",
    "            samples, rate = torchaudio.load(p, normalize=True)\n",
    "            \n",
    "            if samples.shape[0] == 2: samples = samples[0, :]\n",
    "            else: samples = samples[0]\n",
    "            if(len(samples) < 24000):\n",
    "                samples = torch.cat((samples, torch.tensor([0])))\n",
    "\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "            \n",
    "            samples = torch.tensor(noisereduce.reduce_noise(y=samples, sr=rate)) # testing this because I had it on in the ResNet version dataset\n",
    "\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "\n",
    "            coeffs = pywt.wavedec(samples, 'db1', level=DWT_LEVELS)\n",
    "\n",
    "            accum = np.abs(np.array([coeffs[-1]]))\n",
    "            for i in range(DWT_LEVELS - 1):\n",
    "                current_coef = coeffs[DWT_LEVELS - 1 - i]\n",
    "                r = np.abs(np.array([np.repeat(current_coef, pow(2, i + 1))]))\n",
    "                r = r[:, 0:rate]\n",
    "                accum = np.concatenate([accum, r])\n",
    "            \n",
    "            timestep_skip = rate//DWT_TIMESTEPS\n",
    "\n",
    "            channels = [[] for i in range(DWT_LEVELS)]\n",
    "            for i in range(rate//timestep_skip):\n",
    "                for j in range(DWT_LEVELS):\n",
    "                    channels[j].append(accum[j][i*timestep_skip])\n",
    "\n",
    "            channels = torch.tensor(channels)\n",
    "            channels = (channels - channels.min()) / (channels.max()-channels.min())\n",
    "\n",
    "            # convert waveform to spikes\n",
    "            timesteps = channels.shape[1]\n",
    "            waveform_timestep_len = (24000//timesteps)\n",
    "\n",
    "            ts_waveform = [] # timesteps but compressed to time resolution of spectrogram output\n",
    "            current_t = waveform_timestep_len\n",
    "            while current_t <= 24000:\n",
    "                ts_waveform.append(samples[current_t-waveform_timestep_len: current_t].max())\n",
    "                current_t += waveform_timestep_len\n",
    "\n",
    "            if len(ts_waveform) < timesteps: # pad so dimensions match\n",
    "                ts_waveform.append(0) \n",
    "\n",
    "            # normalize because amplitude scales vary a lot (and we did this in resnet)\n",
    "            ts_waveform = torch.tensor(ts_waveform)\n",
    "            ts_waveform = (ts_waveform - ts_waveform.min()) / (ts_waveform.max() - ts_waveform.min())\n",
    "\n",
    "            # debug\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "            #plt.plot(np.linspace(0, len(ts_waveform), len(ts_waveform)), ts_waveform)\n",
    "\n",
    "            # convert compressed waveform into spikes using bins\n",
    "            waveform_spikes = []\n",
    "            num_bins = 20\n",
    "            bin_size = 1/num_bins\n",
    "            for w in ts_waveform:\n",
    "                waveform_spikes.append([0 for i in range(num_bins)])\n",
    "                waveform_spikes[-1][int(w//bin_size)] = 1\n",
    "\n",
    "            waveform_spikes = torch.tensor(waveform_spikes)\n",
    "\n",
    "            # debug\n",
    "            # fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "            # ax = fig.add_subplot(111)\n",
    "            # splt.raster(waveform_spikes, ax, s=5, c=\"black\")\n",
    "\n",
    "            all_spikes.append(channels)\n",
    "            all_waveforms.append(waveform_spikes)\n",
    "\n",
    "        # # global max/min normalization\n",
    "        # print('Looking for global max/min...')\n",
    "        # global_min = 0\n",
    "        # global_max = 0\n",
    "        # for s in all_spikes:\n",
    "        #     if s.min() < global_min: global_min = s.min()\n",
    "        #     if s.max() > global_max: global_max = s.max()\n",
    "\n",
    "        # # now normalize everything\n",
    "        # print('Normalizing...')\n",
    "        # for i in range(len(all_spikes)):\n",
    "        #     all_spikes[i] = (all_spikes[i]-global_min) / (global_max - global_min)\n",
    "\n",
    "        all_spikes = torch.stack(all_spikes)\n",
    "        all_waveforms = torch.stack(all_waveforms)\n",
    "\n",
    "        # delta modulate spectrogram\n",
    "        all_spikes = all_spikes.permute(2, 0, 1)\n",
    "        print('Running deltamod')\n",
    "        all_spikes_dm = posneg_delta(all_spikes)\n",
    "\n",
    "        # note: here, all_spikes_dm is (timestep, batch, neuron) and all_waveforms is (batch, timestep, neuron)\n",
    "        all_waveforms = all_waveforms.permute(1, 0, 2)\n",
    "\n",
    "        return torch.cat((all_spikes_dm, all_waveforms), dim=2), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20835810-da53-4388-b038-5accdd655f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100 samples.\n",
      "Running deltamod\n"
     ]
    }
   ],
   "source": [
    "spikes, labels = to_spikes([i[0] for i in pairs], [i[1] for i in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09748309-0836-4d0f-b153-8b3b78564e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/validation/test split and save\n",
    "train_cutoff_index = int(DATASET_CAP*0.8)\n",
    "test_val_cutoff_offset = int(DATASET_CAP*0.1)\n",
    "training_spikes = spikes[:, 0:train_cutoff_index, :]\n",
    "training_labels = labels[0:train_cutoff_index]\n",
    "training_gunshot_data = []\n",
    "\n",
    "validation_spikes = spikes[:, train_cutoff_index:train_cutoff_index+test_val_cutoff_offset, :]\n",
    "validation_labels = labels[train_cutoff_index:train_cutoff_index+test_val_cutoff_offset]\n",
    "validation_gunshot_data = []\n",
    "validation_filenames = [i[0] for i in pairs][train_cutoff_index:train_cutoff_index+test_val_cutoff_offset]\n",
    "\n",
    "test_spikes = spikes[:, train_cutoff_index+test_val_cutoff_offset:, :]\n",
    "test_labels = labels[train_cutoff_index+test_val_cutoff_offset:]\n",
    "\n",
    "# metadata = [f'dm-threshold: {DM_THRESHOLD}', 'mode: dwt', 'format: snntorch', 'other: per sample norm just like resnet, max pool waveform']\n",
    "# write_spikes_to_disk(FILEPATH, metadata, training_spikes, training_labels, training_gunshot_data,\n",
    "# validation_spikes, validation_labels, validation_gunshot_data, validation_filenames, test_spikes, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eafd0a1-a75d-412b-9b5b-27efaceff5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize output\n",
    "INDEX = 1\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "splt.raster(training_spikes[:, INDEX, :], ax, s=1, c=\"black\")\n",
    "print(labels[INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69195a2-7325-4b95-b0bf-d40537e55381",
   "metadata": {},
   "source": [
    "# Convolution Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dac6b-74fb-4bef-9c4f-c9c6ab5d7d44",
   "metadata": {},
   "source": [
    "To use, generate train, val, test sets using cells above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5261eb69-b7d5-4977-85a7-0f60c2693036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "WINDOW_SIZE = 50\n",
    "HOP_LENGTH = 25\n",
    "TIMESTEPS = training_spikes.shape[0]\n",
    "FILEPATH = './data/120-dwt-conv.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626be468-8683-424f-9ec5-074001972874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expects shape timesteps x batch x neuron\n",
    "def make_windows(dataset, debug=False):\n",
    "    frames = []\n",
    "    for i in range(TIMESTEPS//HOP_LENGTH+1):\n",
    "        if debug: print(f'Frame {i} goes from', i*HOP_LENGTH, 'to', i*HOP_LENGTH+WINDOW_SIZE)\n",
    "        frames.append(dataset[i*HOP_LENGTH:i*HOP_LENGTH+WINDOW_SIZE, :, :])\n",
    "    \n",
    "        # if we need to pad\n",
    "        if i*HOP_LENGTH+WINDOW_SIZE > TIMESTEPS:\n",
    "            pad = torch.zeros(i*HOP_LENGTH+WINDOW_SIZE - TIMESTEPS, dataset.shape[1], dataset.shape[2])\n",
    "            frames[-1] = torch.cat((frames[-1], pad), dim=0)\n",
    "\n",
    "    return torch.stack(frames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ccf46c-ee8d-47cf-a79f-840c1925fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_spikes_w = make_windows(training_spikes)\n",
    "validation_spikes_w = make_windows(validation_spikes)\n",
    "test_spikes_w = make_windows(test_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e4526f-788c-4adf-8f0c-fab729716b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "metadata = [f'dm-threshold: {DM_THRESHOLD}', 'mode: dwt', 'format: snntorch', 'other: nothing']\n",
    "write_spikes_to_disk(FILEPATH, metadata, training_spikes_w, training_labels, training_gunshot_data,\n",
    "validation_spikes_w, validation_labels, validation_gunshot_data, validation_filenames, test_spikes_w, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98d271-c23c-4ec8-9710-58914a7c7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for viewing \n",
    "for f in training_spikes_w[:, :, 1, :]:\n",
    "    fig = plt.figure(facecolor=\"w\", figsize=(3, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    splt.raster(f, ax, s=1, c=\"black\")\n",
    "    plt.ylim(0, 35)\n",
    "    plt.xlim(0, WINDOW_SIZE) # to properly show padding\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3f67a-2545-435d-a8b6-de828f413ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
