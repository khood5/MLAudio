{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893dd921-1942-4e44-83ae-1dab1fe06d15",
   "metadata": {},
   "source": [
    "# Setup\n",
    "This notebook is basically a mix of make_dataset.py from ../snn with encoding using delta modulation and adding waveform data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdec943e-84a4-407f-a115-0f704cbb44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import speech2spikes\n",
    "import torchaudio\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "import noisereduce\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b823c3e3-b02f-4383-b17b-32261bc229dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables for encoding\n",
    "MODE = 'spec' # spec, dwt\n",
    "DATASET_CAP = 40\n",
    "\n",
    "SPEC_FREQ_BIN_COUNT = 25 # for spec mode\n",
    "\n",
    "PATH_GUNSHOT_SOUNDS = '/home/joao/dev/MLAudio/shotspotter/data/gunshotsNew'\n",
    "PATH_GUNSHOT_INDEX = '/home/joao/dev/MLAudio/shotspotter/data/gunshotsNewIndex.csv'\n",
    "PATH_NOGUNSHOT_SOUNDS = '/home/joao/dev/MLAudio/shotspotter/data/genBackgrounds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2c17a0ca-5d74-4c81-9f86-df2c8f65b4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 20 gunshot audio files\n",
      "We have 20 background only audio files\n"
     ]
    }
   ],
   "source": [
    "gunshot_file_paths = [PATH_GUNSHOT_SOUNDS+'/'+fn for fn in os.listdir(PATH_GUNSHOT_SOUNDS)][:DATASET_CAP//2]\n",
    "print(f'We have {len(gunshot_file_paths)} gunshot audio files')\n",
    "nogunshot_file_paths = [PATH_NOGUNSHOT_SOUNDS+'/'+fn for fn in os.listdir(PATH_NOGUNSHOT_SOUNDS)][:DATASET_CAP//2]\n",
    "print(f'We have {len(nogunshot_file_paths)} background only audio files')\n",
    "\n",
    "p1 = [(i, 1) for i in gunshot_file_paths]\n",
    "p2 = [(i, 0) for i in nogunshot_file_paths]\n",
    "\n",
    "pairs = p1+p2 # path to sound - label tuples\n",
    "random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3ca59-566a-410a-850f-22f2766141ea",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "61ced40e-12b6-40dc-ac71-470a8f596563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is shape time x batch x channels (from spectrogram or dwt spec)\n",
    "def posneg_delta(raw_spec_data):\n",
    "    delta = spikegen.delta(raw_spec_data, threshold=0.001, off_spike=True)\n",
    "    \n",
    "    new_data = torch.zeros(delta.shape[0], delta.shape[1], delta.shape[2]*2) # 2 channels for each freq channel\n",
    "    for i in range(delta.shape[1]): # for each sample\n",
    "        for timestep in range(delta.shape[0]):\n",
    "            for channel in range(delta.shape[2]):\n",
    "                if delta[timestep, i, channel] == 1:\n",
    "                    new_data[timestep, i, channel] = 1\n",
    "                elif delta[timestep, i, channel] == -1:\n",
    "                    new_data[timestep, i, channel+delta.shape[2]] = 1;\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def to_spikes(paths_list, labels):\n",
    "    if MODE == 'spec':\n",
    "        all_spikes = []\n",
    "        targets = np.array(labels)\n",
    "\n",
    "        for p in paths_list:\n",
    "            samples, rate = torchaudio.load(p, normalize=True)\n",
    "            \n",
    "            if samples.shape[0] == 2: samples = samples[0, :]\n",
    "            else: samples = samples[0]\n",
    "            if(len(samples) < 24000):\n",
    "                samples = torch.cat((samples, torch.tensor([0])))\n",
    "\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "            \n",
    "            samples = torch.tensor(noisereduce.reduce_noise(y=samples, sr=rate)) # testing this because I had it on in the ResNet version dataset\n",
    "\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "\n",
    "            # freq bin count is nfft//2 + 1\n",
    "            spec_transform = torchaudio.transforms.Spectrogram(n_fft=(2*SPEC_FREQ_BIN_COUNT-2))\n",
    "\n",
    "            samples = samples.to(torch.float64)\n",
    "            spec = spec_transform(samples)\n",
    "\n",
    "            # convert waveform to spikes\n",
    "            timesteps = spec.shape[1]\n",
    "            waveform_timestep_len = (24000//timesteps) + 1\n",
    "\n",
    "            ts_waveform = [] # timesteps but compressed to time resolution of spectrogram output\n",
    "            current_t = waveform_timestep_len\n",
    "            while current_t <= 24000:\n",
    "                ts_waveform.append(samples[current_t-waveform_timestep_len: current_t].mean())\n",
    "                current_t += waveform_timestep_len\n",
    "\n",
    "            if len(ts_waveform) < timesteps: # pad so dimensions match\n",
    "                ts_waveform.append(0) \n",
    "\n",
    "            # normalize because amplitude scales vary a lot (and we did this in resnet accidentally)\n",
    "            ts_waveform = torch.tensor(ts_waveform)\n",
    "            ts_waveform = (ts_waveform - ts_waveform.min()) / (ts_waveform.max() - ts_waveform.min())\n",
    "\n",
    "            # debug\n",
    "            #plt.plot(np.linspace(0, len(samples), len(samples)), samples)\n",
    "            #plt.plot(np.linspace(0, len(ts_waveform), len(ts_waveform)), ts_waveform)\n",
    "\n",
    "            # convert compressed waveform into spikes using bins\n",
    "            waveform_spikes = []\n",
    "            num_bins = 20\n",
    "            bin_size = 1/num_bins\n",
    "            for w in ts_waveform:\n",
    "                waveform_spikes.append([0 for i in range(num_bins)])\n",
    "                waveform_spikes[-1][int(w//bin_size)] = 1\n",
    "\n",
    "            waveform_spikes = torch.tensor(waveform_spikes)\n",
    "            splt.raster()\n",
    "\n",
    "            ## CONITNUE HERE\n",
    "            break\n",
    "            all_spikes.append(spec)\n",
    "\n",
    "        # same as dwt\n",
    "        global_min = 0\n",
    "        global_max = 0\n",
    "        for s in all_spikes:\n",
    "            if s.min() < global_min: global_min = s.min()\n",
    "            if s.max() > global_max: global_max = s.max()\n",
    "\n",
    "        # now normalize everything\n",
    "        for i in range(len(all_spikes)):\n",
    "            all_spikes[i] = (all_spikes[i]-global_min) / (global_max - global_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "20835810-da53-4388-b038-5accdd655f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1001, 20])\n"
     ]
    }
   ],
   "source": [
    "to_spikes([i[0] for i in pairs], [i[1] for i in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafd0a1-a75d-412b-9b5b-27efaceff5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
