{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from audioDataLoader import audioDataloader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import neuro\n",
    "from py_apps import utils\n",
    "from py_apps.utils.common_utils import read_network\n",
    "from py_apps.utils.common_utils import load_json_arg\n",
    "from py_apps.utils.neuro_help import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ## Time Series Datasets\n",
    ">\n",
    ">classify_driver.py also supports timeseries data for classification. In this case, you will be using the app_type load and setting --timeseries true. <br>\n",
    ">For datasets stored in this manner, if your data is stored in three-dimensional array A, then A[i] gives the first data  <br>\n",
    ">instance, and A[i][j] will give the j<sup>th</sup> feature of the i<sup>th</sup> data instance, and A[i][j][k] gives the k<sup>th</sup> timestep of the j<sup>th</sup> feature of the i<sup>th</sup> data  <br>\n",
    ">instance. An example of a time series data set is given in the data directory, the Activity Recognition system based on  <br>\n",
    ">Multisensor data fusion (AReM) Data Set from the UCI repository. In this example, we are also using a custom encoder (with --encoder) and setting a  <br>\n",
    ">new simulation time (with --sim_time), along with using multiple processes to train in parallel (with --processes).  <br>\n",
    "<br>\n",
    "\n",
    "## Summary \n",
    "<ul>\n",
    "    <li>i: Index of a data instance. A[i] gives the first, A[i+1] the second, and so on.</li>\n",
    "    <li>j: Feature index within a data instance. A[i][j] gives the j<sup>th</sup> feature.</li>\n",
    "    <li>k: Timestep of a feature within a data instance. A[i][j][k] gets the i<sup>th</sup> data instance's j<sup>th</sup> feature's value at the k<sup>th</sup> timestep.</li>\n",
    "</ul>\n",
    "In summary:\n",
    "\n",
    "    A[i] represents the ith data instance.\n",
    "    A[i][j] represents the jth feature of the ith data instance.\n",
    "    A[i][j][k] represents the kth timestep of the jth feature of the ith data instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing\n",
    "You can test diffrent encoding paramiters with the <strong>spike_encoder_util</strong> from the framework repo. Utils programs are useful for testing and troubleshooting. \n",
    "\n",
    "<b>To make the util prgrames you will need to</b>\n",
    "\n",
    "<ol>\n",
    "    <li>checkout the framework repo </li>\n",
    "    <li>Include the following flag in the makefile, positioned either under the line \"Build Flags\": <code>CFLAGS ?= -pthread</code></li>\n",
    "    <li>run <strong>make utils</strong> from the root of the framework repo</li>\n",
    "</ol>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then run any of the util programs from the root of the framework repo with <code>./bin/< < UTIL PROGRAM NAME > > '< < PROMPT > >'</code><br>\n",
    "for example:<br>\n",
    "<pre>\n",
    "UNIX> pwd\n",
    "/Users/USERNAME/Documents/framework\n",
    "UNIX> <b>./bin/spike_encoder_tool 'SE>'</b>\n",
    "SE> ?                                                                   # This prints out all of the options.\n",
    "For commands that take a json either put a filename on the same line,\n",
    "or the json can be multiple lines, starting on the next line.\n",
    "\n",
    "FJ json                - Read a spike encoder from json.\n",
    "TJ                     - Create JSON from the spike encoder.\n",
    "PAS                    - Call print_all_settings() for debugging.\n",
    "SOI int [over]         - Call set_overall_interval(interval, override=true).\n",
    "I                      - Call get_overall_interval().\n",
    "MS                     - Call get_max_spikes().\n",
    "N                      - Return the number of neurons for the spike encoder.\n",
    "GS val min max         - Get spikes for the given val, whose range is [min,max].\n",
    "GTS vals min max       - Get timeseries spikes for the given vals, whose range is [min,max].\n",
    "GSS vals times min max - Get sparse spikes for the given vals and times, and val range is [min,max] \n",
    "\n",
    "?                      - Print commands.\n",
    "Q                      - Quit.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"dataset_15sec/train/train.csv\"\n",
    "valid_dataset = \"dataset_15sec/valid/valid.csv\"\n",
    "test_dataset = \"dataset_15sec/test/test.csv\"\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[2.3009], std=[42.1936]) \n",
    "    ])\n",
    "\n",
    "train_data = audioDataloader(index_file=train_dataset, transforms=data_transform)\n",
    "valid_data = audioDataloader(index_file=valid_dataset, transforms=data_transform)\n",
    "test_data = audioDataloader(index_file=test_dataset, transforms=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_tensor = torch.transpose(train_data[0][0], 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7201, 201])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.zeros([2,201,7201])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3665,  0.5570,  1.2286,  ..., -0.0420, -0.0333, -0.0505],\n",
       "        [ 2.2995,  0.1478,  0.1014,  ..., -0.0409, -0.0432, -0.0512],\n",
       "        [ 1.3559,  0.0761,  0.4851,  ..., -0.0503, -0.0539, -0.0542],\n",
       "        ...,\n",
       "        [-0.0543, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] = train_data[0][0]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1] = train_data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 201, 7201])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3665,  0.5570,  1.2286,  ..., -0.0420, -0.0333, -0.0505],\n",
       "        [ 2.2995,  0.1478,  0.1014,  ..., -0.0409, -0.0432, -0.0512],\n",
       "        [ 1.3559,  0.0761,  0.4851,  ..., -0.0503, -0.0539, -0.0542],\n",
       "        ...,\n",
       "        [-0.0543, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 7201])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3665,  0.5570,  1.2286,  ..., -0.0420, -0.0333, -0.0505])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 0\n",
    "data[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7201])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i][j].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.366456"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "data.numpy()[i][j][k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A[i] represents the ith data instance.\n",
    "    A[i][j] represents the jth feature of the ith data instance.\n",
    "    A[i][j][k] represents the kth timestep of the jth feature of the ith data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2 data instance\n",
      "there are 201 features of each data instance\n",
      "there are 7201 timestep in each feature of each data instance\n"
     ]
    }
   ],
   "source": [
    "print(f\"there are {len(data)} data instance\")\n",
    "print(f\"there are {len(data[i])} features of each data instance\")\n",
    "print(f\"there are {len(data[i][j])} timestep in each feature of each data instance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 201, 7201)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"dataset_15sec/train/train.csv\"\n",
    "valid_dataset = \"dataset_15sec/valid/valid.csv\"\n",
    "test_dataset = \"dataset_15sec/test/test.csv\"\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[2.3009], std=[42.1936]) \n",
    "    ])\n",
    "\n",
    "train_data = audioDataloader(index_file=train_dataset, transforms=data_transform)\n",
    "valid_data = audioDataloader(index_file=valid_dataset, transforms=data_transform)\n",
    "test_data = audioDataloader(index_file=test_dataset, transforms=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_instance_dim = len(train_data)\n",
    "data_instance_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dim = len(train_data[0][0][0])\n",
    "features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7201"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep_dim = len(train_data[0][0][0][0])\n",
    "timestep_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 7201])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
