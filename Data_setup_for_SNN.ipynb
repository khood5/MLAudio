{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from audioDataLoader import audioDataloader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import neuro\n",
    "from py_apps import utils\n",
    "from py_apps.utils.common_utils import read_network\n",
    "from py_apps.utils.common_utils import load_json_arg\n",
    "from py_apps.utils.neuro_help import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60.        , 60.        , 60.        , ..., 60.00069713,\n",
       "        60.00069605, 60.00069504],\n",
       "       [60.        , 60.        , 60.        , ..., 60.00070597,\n",
       "        60.00070482, 60.00070367],\n",
       "       [60.        , 60.        , 60.        , ..., 60.00070733,\n",
       "        60.00070613, 60.0007049 ],\n",
       "       ...,\n",
       "       [60.        , 60.        , 60.        , ..., 60.0007285 ,\n",
       "        60.00072819, 60.00072784],\n",
       "       [60.        , 60.        , 60.        , ..., 60.00073347,\n",
       "        60.0007332 , 60.00073288],\n",
       "       [60.        , 60.        , 60.        , ..., 60.00072845,\n",
       "        60.00072809, 60.00072769]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old data format \n",
    "# File path\n",
    "file_path = \"/data2/khood/PowerGridData_training.npy\"\n",
    "data = np.load(file_path)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "ea_json = {}\n",
    "ea_json[\"dmin\"] = [float(np.min(X[:,i])) for i in range(len(X[0]))]\n",
    "ea_json[\"dmax\"] = [float(np.max(X[:,i])) for i in range(len(X[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60.00048582, 60.00020592, 60.00015613, 60.00016548, 60.00019947,\n",
       "       60.00019522, 60.00023037, 60.00024782, 60.00052549, 60.00015386,\n",
       "       60.00016741, 60.00015825, 60.00014899, 60.00061492, 60.00005609,\n",
       "       60.00002154, 60.00007367, 60.00010527, 59.99993716, 59.99990395,\n",
       "       60.00045922, 59.99993634, 59.99993531, 60.0000094 , 60.00017263,\n",
       "       60.00011805, 60.00010768, 60.00009439, 60.00008629, 60.0009906 ,\n",
       "       60.00051745, 60.0005559 , 60.00057731, 60.00061758, 60.00063709,\n",
       "       60.00061363, 60.00065301, 60.00055303, 60.00067229, 60.00062938,\n",
       "       60.00089973, 60.00096554, 60.00066497, 60.0006655 , 60.00068863,\n",
       "       60.00060792, 60.00053278, 60.0005675 , 60.00066436, 60.00081466,\n",
       "       60.0007265 , 60.00092733, 60.00019853, 60.00061951, 60.0001098 ,\n",
       "       59.99990389, 59.99985908, 59.99989436, 59.99987661, 60.00013398,\n",
       "       60.00006871, 60.00051375, 60.00054503, 60.00061353, 60.0006794 ,\n",
       "       60.00090236, 60.00096811, 60.00094049])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.96341599826725\n",
      "60.073241081442056\n"
     ]
    }
   ],
   "source": [
    "print(ea_json[\"dmin\"][0])\n",
    "print(ea_json[\"dmax\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ## Time Series Datasets\n",
    ">\n",
    ">classify_driver.py also supports timeseries data for classification. In this case, you will be using the app_type load and setting --timeseries true. <br>\n",
    ">For datasets stored in this manner, if your data is stored in three-dimensional array A, then A[i] gives the first data  <br>\n",
    ">instance, and A[i][j] will give the j<sup>th</sup> feature of the i<sup>th</sup> data instance, and A[i][j][k] gives the k<sup>th</sup> timestep of the j<sup>th</sup> feature of the i<sup>th</sup> data  <br>\n",
    ">instance. An example of a time series data set is given in the data directory, the Activity Recognition system based on  <br>\n",
    ">Multisensor data fusion (AReM) Data Set from the UCI repository. In this example, we are also using a custom encoder (with --encoder) and setting a  <br>\n",
    ">new simulation time (with --sim_time), along with using multiple processes to train in parallel (with --processes).  <br>\n",
    "<br>\n",
    "\n",
    "## Summary \n",
    "<ul>\n",
    "    <li>i: Index of a data instance. A[i] gives the first, A[i+1] the second, and so on.</li>\n",
    "    <li>j: Feature index within a data instance. A[i][j] gives the j<sup>th</sup> feature.</li>\n",
    "    <li>k: Timestep of a feature within a data instance. A[i][j][k] gets the i<sup>th</sup> data instance's j<sup>th</sup> feature's value at the k<sup>th</sup> timestep.</li>\n",
    "</ul>\n",
    "In summary:\n",
    "\n",
    "    A[i] represents the ith data instance.\n",
    "    A[i][j] represents the jth feature of the ith data instance.\n",
    "    A[i][j][k] represents the kth timestep of the jth feature of the ith data instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing\n",
    "You can test diffrent encoding paramiters with the <strong>spike_encoder_util</strong> from the framework repo. Utils programs are useful for testing and troubleshooting. \n",
    "\n",
    "<b>To make the util prgrames you will need to</b>\n",
    "\n",
    "<ol>\n",
    "    <li>checkout the framework repo </li>\n",
    "    <li>Include the following flag in the makefile, positioned either under the line \"Build Flags\": <code>CFLAGS ?= -pthread</code></li>\n",
    "    <li>run <strong>make utils</strong> from the root of the framework repo</li>\n",
    "</ol>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then run any of the util programs from the root of the framework repo with <code>./bin/< < UTIL PROGRAM NAME > > '< < PROMPT > >'</code><br>\n",
    "for example:<br>\n",
    "<pre>\n",
    "UNIX> pwd\n",
    "/Users/USERNAME/Documents/framework\n",
    "UNIX> <b>./bin/spike_encoder_tool 'SE>'</b>\n",
    "SE> ?                                                                   # This prints out all of the options.\n",
    "For commands that take a json either put a filename on the same line,\n",
    "or the json can be multiple lines, starting on the next line.\n",
    "\n",
    "FJ json                - Read a spike encoder from json.\n",
    "TJ                     - Create JSON from the spike encoder.\n",
    "PAS                    - Call print_all_settings() for debugging.\n",
    "SOI int [over]         - Call set_overall_interval(interval, override=true).\n",
    "I                      - Call get_overall_interval().\n",
    "MS                     - Call get_max_spikes().\n",
    "N                      - Return the number of neurons for the spike encoder.\n",
    "GS val min max         - Get spikes for the given val, whose range is [min,max].\n",
    "GTS vals min max       - Get timeseries spikes for the given vals, whose range is [min,max].\n",
    "GSS vals times min max - Get sparse spikes for the given vals and times, and val range is [min,max] \n",
    "\n",
    "?                      - Print commands.\n",
    "Q                      - Quit.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"dataset_15sec/train/train.csv\"\n",
    "valid_dataset = \"dataset_15sec/valid/valid.csv\"\n",
    "test_dataset = \"dataset_15sec/test/test.csv\"\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[2.3009], std=[42.1936]) \n",
    "    ])\n",
    "\n",
    "train_data = audioDataloader(index_file=train_dataset, transforms=data_transform)\n",
    "valid_data = audioDataloader(index_file=valid_dataset, transforms=data_transform)\n",
    "test_data = audioDataloader(index_file=test_dataset, transforms=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 7201])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.zeros([2,201,7201])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3665,  0.5570,  1.2286,  ..., -0.0420, -0.0333, -0.0505],\n",
       "        [ 2.2995,  0.1478,  0.1014,  ..., -0.0409, -0.0432, -0.0512],\n",
       "        [ 1.3559,  0.0761,  0.4851,  ..., -0.0503, -0.0539, -0.0542],\n",
       "        ...,\n",
       "        [-0.0543, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0] = train_data[0][0]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1] = train_data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 201, 7201])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3665,  0.5570,  1.2286,  ..., -0.0420, -0.0333, -0.0505],\n",
       "        [ 2.2995,  0.1478,  0.1014,  ..., -0.0409, -0.0432, -0.0512],\n",
       "        [ 1.3559,  0.0761,  0.4851,  ..., -0.0503, -0.0539, -0.0542],\n",
       "        ...,\n",
       "        [-0.0543, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545],\n",
       "        [-0.0544, -0.0545, -0.0545,  ..., -0.0545, -0.0545, -0.0545]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 7201])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3665,  0.5570,  1.2286,  ..., -0.0420, -0.0333, -0.0505])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 0\n",
    "data[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7201])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[i][j].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3665)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "data[i][j][k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A[i] represents the ith data instance.\n",
    "    A[i][j] represents the jth feature of the ith data instance.\n",
    "    A[i][j][k] represents the kth timestep of the jth feature of the ith data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2 data instance\n",
      "there are 201 features of each data instance\n",
      "there are 7201 timestep in each feature of each data instance\n"
     ]
    }
   ],
   "source": [
    "print(f\"there are {len(data)} data instance\")\n",
    "print(f\"there are {len(data[i])} features of each data instance\")\n",
    "print(f\"there are {len(data[i][j])} timestep in each feature of each data instance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"dataset_15sec/train/train.csv\"\n",
    "valid_dataset = \"dataset_15sec/valid/valid.csv\"\n",
    "test_dataset = \"dataset_15sec/test/test.csv\"\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[2.3009], std=[42.1936]) \n",
    "    ])\n",
    "\n",
    "train_data = audioDataloader(index_file=train_dataset, transforms=data_transform)\n",
    "valid_data = audioDataloader(index_file=valid_dataset, transforms=data_transform)\n",
    "test_data = audioDataloader(index_file=test_dataset, transforms=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_instance_dim = len(train_data)\n",
    "data_instance_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dim = len(train_data[0][0][0])\n",
    "features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7201"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep_dim = len(train_data[0][0][0][0])\n",
    "timestep_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making training numpy file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 550/40000 [01:33<1:51:21,  5.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m neuroTrain \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((data_instance_dim, features_dim, timestep_dim))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(data_instance_dim)):\n\u001b[0;32m----> 4\u001b[0m     neuroTrain[i] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m neuroTrain \u001b[38;5;241m=\u001b[39m neuroTrain\u001b[38;5;241m.\u001b[39mnumpy\n\u001b[1;32m      6\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneuroTrain.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, neuroTrain)\n",
      "File \u001b[0;32m/data2/khood/GitHub/MLAudio/audioDataLoader.py:38\u001b[0m, in \u001b[0;36maudioDataloader.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     36\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(filename, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m mono_waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Convert stereo to mono\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m specgram_transform \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSpectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmono_waveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Apply additional transforms if provided\u001b[39;00m\n\u001b[1;32m     40\u001b[0m specgram_transform \u001b[38;5;241m=\u001b[39m specgram_transform\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# transform from shape (Hight,Width) to (Channel,Hight,Width)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlaudio/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlaudio/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlaudio/lib/python3.12/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlaudio/lib/python3.12/site-packages/torchaudio/functional/functional.py:117\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    109\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`return_complex` argument is now deprecated and is not effective.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchaudio.functional.spectrogram(power=None)` always returns a tensor with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex dtype. Please remove the argument in the function call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# TODO add \"with torch.no_grad():\" back when JIT supports it\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m frame_length_norm, window_norm \u001b[38;5;241m=\u001b[39m _get_spec_norms(normalized)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# pack batch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlaudio/lib/python3.12/site-packages/torch/nn/functional.py:4495\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4488\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   4489\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   4490\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4491\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4492\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplication_pad2d(\n\u001b[1;32m   4493\u001b[0m                 \u001b[38;5;28minput\u001b[39m, pad\n\u001b[1;32m   4494\u001b[0m             )\n\u001b[0;32m-> 4495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"making training numpy file\")\n",
    "neuroTrain = torch.zeros((data_instance_dim, features_dim, timestep_dim))\n",
    "for i in tqdm(range(data_instance_dim)):\n",
    "    neuroTrain[i] = train_data[i][0]\n",
    "neuroTrain = neuroTrain.numpy\n",
    "np.save('neuroTrain.npy', neuroTrain)\n",
    "\n",
    "print(\"making validation numpy file\")\n",
    "data_instance_dim = len(valid_data)\n",
    "neuroValid = torch.zeros((data_instance_dim, features_dim, timestep_dim))\n",
    "for i in tqdm(range(data_instance_dim)):\n",
    "    neuroValid[i] = valid_data[i][0]\n",
    "neuroValid = neuroValid.numpy\n",
    "np.save('neuroValid.npy', neuroValid)\n",
    "\n",
    "print(\"Making testing numpy file\")\n",
    "data_instance_dim = len(test_data)\n",
    "neuroTest = torch.zeros((data_instance_dim, features_dim, timestep_dim))\n",
    "for i in tqdm(range(data_instance_dim)):\n",
    "    neuroTest[i] = test_data[i][0]\n",
    "neuroTest = neuroTest.numpy\n",
    "np.save('neuroTest.npy', neuroTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
