{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khood/anaconda3/envs/mlaudio/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import csv\n",
    "import torchaudio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri /data/khood/GitHub/MLAudio/dataset/background/background_0a0a8446-5d0e-4e4d-9f2b-daf3d64ab5e8.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/khood/GitHub/MLAudio/dataset/background/background_0a0a8446-5d0e-4e4d-9f2b-daf3d64ab5e8.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m mono_waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Convert stereo to mono\u001b[39;00m\n\u001b[1;32m      3\u001b[0m specgram_transform \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mSpectrogram()(mono_waveform)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlaudio/lib/python3.12/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlaudio/lib/python3.12/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri /data/khood/GitHub/MLAudio/dataset/background/background_0a0a8446-5d0e-4e4d-9f2b-daf3d64ab5e8.wav and format None."
     ]
    }
   ],
   "source": [
    "waveform, sample_rate = torchaudio.load(\"/data/khood/GitHub/MLAudio/dataset/background/background_0a0a8446-5d0e-4e4d-9f2b-daf3d64ab5e8.wav\")\n",
    "mono_waveform = waveform.mean(dim=0)  # Convert stereo to mono\n",
    "specgram_transform = torchaudio.transforms.Spectrogram()(mono_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate2, samples = wavfile.read(\"/data/khood/GitHub/MLAudio/dataset/background/background_0a0a8446-5d0e-4e4d-9f2b-daf3d64ab5e8.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(\"/data/khood/GitHub/MLAudio/dataset/background/background_0a0a8446-5d0e-4e4d-9f2b-daf3d64ab5e8.wav\", normalize=True)\n",
    "mono_waveform = waveform.mean(dim=0)  # Convert stereo to mono\n",
    "specgram_transform = torchaudio.transforms.Spectrogram(pad=10)(mono_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8142e-01, 4.9903e-01, 2.9857e-01,  ..., 2.5468e+02, 2.1162e+02,\n",
       "         1.8403e+01],\n",
       "        [1.0262e-01, 2.3846e-01, 2.1369e-01,  ..., 8.1516e+01, 5.9499e+01,\n",
       "         1.8949e+00],\n",
       "        [1.1044e-02, 1.0026e-01, 9.9514e-02,  ..., 2.9945e+00, 8.1965e-01,\n",
       "         5.9924e+00],\n",
       "        ...,\n",
       "        [2.9327e-05, 1.7324e-08, 1.3643e-08,  ..., 4.2028e-07, 5.6712e-08,\n",
       "         2.0261e-06],\n",
       "        [3.1501e-05, 4.1418e-11, 1.3649e-10,  ..., 3.2212e-07, 1.7897e-07,\n",
       "         2.3794e-06],\n",
       "        [3.2280e-05, 9.7363e-10, 8.4259e-10,  ..., 9.8445e-08, 5.0479e-09,\n",
       "         1.7158e-06]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specgram_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0079, -0.0003])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-19464192,  -5046272], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the device to use\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audioDataloader(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_file: str, \n",
    "        header=None,\n",
    "        transforms=None\n",
    "    ):\n",
    "        indexFile = np.array(pd.read_csv(index_file,header=header))\n",
    "        self.audioFiles = indexFile[:, 0]\n",
    "        self.audioLabels = indexFile[:, 1]\n",
    "        self.transforms = transforms\n",
    "    # frequencies   (Array of sample frequencies):\n",
    "    #               This represents the array of sample frequencies, i.e., the frequencies at which the spectrogram is calculated.\n",
    "    #               It corresponds to the y-axis of the spectrogram plot, indicating the different frequency bins.\n",
    "\n",
    "    # times (Array of segment times):\n",
    "    #               This represents the array of segment times, i.e., the time points at which each segment of the spectrogram is calculated.\n",
    "    #               It corresponds to the x-axis of the spectrogram plot, indicating different time points or segments of the signal.\n",
    "\n",
    "    # spectrogram_data (Spectrogram of x):\n",
    "    #               This is the actual spectrogram data, representing the magnitude squared of the signal's frequency content at different time segments.\n",
    "    #               It is a 2D array where the rows correspond to frequency bins (f) and the columns correspond to time segments (t).\n",
    "    #               The intensity of each element in Sxx represents the magnitude of the frequency component at the corresponding frequency and time.\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.audioFiles[index]\n",
    "        if not os.path.exists(filename):\n",
    "            raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(filename, normalize=True)\n",
    "        mono_waveform = waveform.mean(dim=0)  # Convert stereo to mono\n",
    "        specgram_transform = torchaudio.transforms.Spectrogram(pad=10)(mono_waveform)\n",
    "        # Apply additional transforms if provided\n",
    "        if self.transforms is not None:\n",
    "            mono_waveform = self.transforms(mono_waveform)\n",
    "        \n",
    "        label_tensor = torch.tensor([self.audioLabels[index]], dtype=torch.float32)\n",
    "        return (specgram_transform, label_tensor)\n",
    "\n",
    "    def getFilePath(self, index):\n",
    "        return self.audioFiles[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audioLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = audioDataloader(index_file=\"/data/khood/GitHub/MLAudio/dataset/index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2669e+03, 4.4169e+03, 3.5351e+03,  ..., 2.3675e+01, 5.5956e+01,\n",
       "          6.0158e+02],\n",
       "         [3.3614e+02, 1.2306e+03, 9.9287e+02,  ..., 9.7082e+01, 1.4314e+02,\n",
       "          2.2710e+02],\n",
       "         [7.2397e+01, 3.5013e+00, 2.9276e+00,  ..., 1.5340e+02, 8.7110e+01,\n",
       "          2.0501e+00],\n",
       "         ...,\n",
       "         [3.9000e-02, 4.2781e-07, 2.3177e-07,  ..., 8.2779e-06, 3.9366e-06,\n",
       "          2.1210e-02],\n",
       "         [4.1856e-02, 1.5923e-07, 7.9586e-08,  ..., 2.1311e-06, 5.9349e-07,\n",
       "          2.2822e-02],\n",
       "         [4.2859e-02, 7.0431e-07, 4.7279e-08,  ..., 1.8417e-07, 5.7157e-07,\n",
       "          2.3373e-02]]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7.8705e-01, 1.5253e+02, 1.2254e-02,  ..., 1.6294e+00, 1.0889e+00,\n",
       "          1.0590e-01],\n",
       "         [4.1139e+01, 6.0106e+01, 2.8725e+00,  ..., 1.1671e+00, 3.8079e-01,\n",
       "          6.2344e-03],\n",
       "         [3.3439e+02, 8.2917e+01, 4.3786e+01,  ..., 7.5508e-01, 2.5646e-02,\n",
       "          3.1965e-02],\n",
       "         ...,\n",
       "         [2.7232e-02, 1.7836e-05, 1.0976e-05,  ..., 4.1920e-08, 1.7745e-09,\n",
       "          1.1272e-03],\n",
       "         [2.7304e-02, 2.1516e-07, 2.5726e-08,  ..., 1.2690e-08, 1.6668e-09,\n",
       "          1.1888e-03],\n",
       "         [2.7433e-02, 5.9672e-09, 2.1349e-08,  ..., 7.7819e-11, 2.1615e-11,\n",
       "          1.2111e-03]]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 28801])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 28801])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 28801])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 28801])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([201, 28801])]\n",
      "['/data/khood/GitHub/MLAudio/dataset/background/background_d14f0a00-b86b-49bd-85aa-e5d29649d04d.wav']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = [data[0][0].shape]\n",
    "f = [data.getFilePath(1)]\n",
    "for i in tqdm(range(len(data))):\n",
    "    if data[i][0].shape not in s:\n",
    "        s.append(data[i][0].shape)\n",
    "        f.append(data.getFilePath(i))\n",
    "    break\n",
    "print(s)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dataset = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(dataset):\n",
    "    data2, labels = j\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(5789001, 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(100,1),\n",
    "          nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_loss = 0.\n",
    "number_of_epoch = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current loss 0.46:   0%|          | 0/25 [1:13:42<?, ?it/s]              \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m      5\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m audioSample\n\u001b[0;32m----> 6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in (pbar := tqdm(range(number_of_epoch))):\n",
    "    for i, audioSample in enumerate(dataset):\n",
    "        running_loss = 0.\n",
    "        inputs, labels = audioSample\n",
    "        inputs = torch.flatten(inputs, start_dim=1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        l = loss(outputs, labels)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += l.item()\n",
    "        \n",
    "        pbar.set_description(f\"current loss {running_loss / batch_size}\")\n",
    "    losses.append(running_loss / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
